{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "material-richardson",
   "metadata": {},
   "source": [
    "# Train a Graph Neural Network\n",
    "\n",
    "Let's now apply everything you learned about GNNs. We are starting with some imports. `torch` and `numpy` are a must. \n",
    "\n",
    "`torchnet` is an extension of torch, but here we will only include that to handle the graph data. Don't be too concerned about it. You won't going to need it.\n",
    "\n",
    "`networkx` is an amazing library for the creation, manipulation and plotting of graphs. We briefly encountered it when we talked about eigennalues and eigen vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchnet as tnt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-advice",
   "metadata": {},
   "source": [
    "## Load data and graph utils\n",
    "\n",
    "Here we load the MUTAG dataset as a `networkx` graph and transform it to a Pytorch dataset. Each node in the dataset contains a label from 0 to 6 which will be used as a one-hot-encoding feature vector. From the 188 graphs nodes, we will use 150 for training and the rest for validation. We have two classes. Don't focus too much on the data loading part. The important stuff is what comes next.Besides, the goal is to demonstrate that graph neural networks are a great fit for such data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "#add MUTAG data in the environment\n",
    "sys.path.append(cwd + '/../MUTAG')\n",
    "\n",
    "\n",
    "\"\"\" Download MUTAG dataset\"\"\"\n",
    "\"\"\" Extra graph utils and data loading stuff\"\"\"\n",
    "\n",
    "def indices_to_one_hot(number, nb_classes, label_dummy=-1):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    if number == label_dummy:\n",
    "        return np.zeros(nb_classes)\n",
    "    else:\n",
    "        return np.eye(nb_classes)[number]\n",
    "\n",
    "def get_graph_signal(nx_graph):\n",
    "    d = dict((k, v) for k, v in nx_graph.nodes.items())\n",
    "    x = []\n",
    "    invd = {}\n",
    "    j = 0\n",
    "    for k, v in d.items():\n",
    "        x.append(v['attr_dict'])\n",
    "        invd[k] = j\n",
    "        j = j + 1\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "def load_data(path, ds_name, use_node_labels=True, max_node_label=10):\n",
    "    node2graph = {}\n",
    "    Gs = []\n",
    "    data = []\n",
    "    dataset_graph_indicator = f\"{ds_name}_graph_indicator.txt\"\n",
    "    dataset_adj = f\"{ds_name}_A.txt\"\n",
    "    dataset_node_labels = f\"{ds_name}_node_labels.txt\"\n",
    "    dataset_graph_labels = f\"{ds_name}_graph_labels.txt\"\n",
    "\n",
    "    path_graph_indicator = os.path.join(path,dataset_graph_indicator)\n",
    "    path_adj = os.path.join(path,dataset_adj)\n",
    "    path_node_lab = os.path.join(path,dataset_node_labels)\n",
    "    path_labels = os.path.join(path,dataset_graph_labels)\n",
    "\n",
    "\n",
    "    with open(path_graph_indicator, \"r\") as f:\n",
    "        c = 1\n",
    "        for line in f:\n",
    "            node2graph[c] = int(line[:-1])\n",
    "            if not node2graph[c] == len(Gs):\n",
    "                Gs.append(nx.Graph())\n",
    "            Gs[-1].add_node(c)\n",
    "            c += 1\n",
    "\n",
    "    with open(path_adj, \"r\") as f:\n",
    "        for line in f:\n",
    "            edge = line[:-1].split(\",\")\n",
    "            edge[1] = edge[1].replace(\" \", \"\")\n",
    "            Gs[node2graph[int(edge[0])] - 1].add_edge(int(edge[0]), int(edge[1]))\n",
    "\n",
    "    if use_node_labels:\n",
    "        with open(path_node_lab, \"r\") as f:\n",
    "            c = 1\n",
    "            for line in f:\n",
    "                node_label = indices_to_one_hot(int(line[:-1]), max_node_label)\n",
    "                Gs[node2graph[c] - 1].add_node(c, attr_dict=node_label)\n",
    "                c += 1\n",
    "\n",
    "    labels = []\n",
    "    with open(path_labels, \"r\") as f:\n",
    "        for line in f:\n",
    "            labels.append(int(line[:-1]))\n",
    "\n",
    "    return list(zip(Gs, labels)) \n",
    "\n",
    "def create_loaders(dataset, batch_size, split_id, offset=-1):\n",
    "    train_dataset = dataset[:split_id]\n",
    "    val_dataset = dataset[split_id:]\n",
    "    return to_pytorch_dataset(train_dataset, offset,batch_size), to_pytorch_dataset(val_dataset, offset,batch_size)\n",
    "\n",
    "def to_pytorch_dataset(dataset, label_offset=0, batch_size=1):\n",
    "    #graphs, labels = dataset\n",
    "    list_set = []\n",
    "    for graph, label in dataset:\n",
    "        F, G = get_graph_signal(graph), nx.to_numpy_matrix(graph)\n",
    "        numOfNodes = G.shape[0]\n",
    "        F_tensor = torch.from_numpy(F).float()\n",
    "        G_tensor = torch.from_numpy(G).float()\n",
    "\n",
    "        # fix labels to zero-indexing\n",
    "        if label == -1:\n",
    "            label = 0\n",
    "\n",
    "        label += label_offset\n",
    "\n",
    "        list_set.append(tuple((F_tensor, G_tensor, label)))\n",
    "\n",
    "    dataset_tnt = tnt.dataset.ListDataset(list_set)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset_tnt, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_data(path='../MUTAG', ds_name='MUTAG',\n",
    "                  use_node_labels=True, max_node_label=7)\n",
    "train_dataset, val_dataset = create_loaders(dataset, batch_size=1, split_id=150, offset=0)\n",
    "print('Data are ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-ceramic",
   "metadata": {},
   "source": [
    "## GCN Layer\n",
    "\n",
    "Now it's your turn. Below, there is a Pytorch module that represents a GCN layer. Your goal is to build that layer from scratch. For your own help, we provide you with two utility functions that create the normalized laplacian of the graph.\n",
    "\n",
    "Remember that GCNs are nothing more than a matrix multiplication between the input. You will find the sollution is the last cell in this notebooks. But give it a shot. It is just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_as(x,y):\n",
    "    return x.to(y.device)\n",
    "\n",
    "# tensor operationa now support batched inputs\n",
    "def calc_degree_matrix_norm(a):\n",
    "    return torch.diag_embed(torch.pow(a.sum(dim=-1),-0.5))\n",
    "\n",
    "def create_graph_lapl_norm(a):\n",
    "    size = a.shape[-1]\n",
    "    a +=  device_as(torch.eye(size),a)\n",
    "    D_norm = calc_degree_matrix_norm(a)\n",
    "    L_norm = torch.bmm( torch.bmm(D_norm, a) , D_norm )\n",
    "    return L_norm\n",
    "\n",
    "\n",
    "class GCN_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple GCN layer\n",
    "    \"\"\"\n",
    "    \n",
    "    ### 1. BUILD YOUR GCN LAYER HERE\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        A: adjÎ±cency matrix\n",
    "        X: graph signal\n",
    "        \"\"\"\n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-heaven",
   "metadata": {},
   "source": [
    "## Graph Neural Network\n",
    "\n",
    "Now let's stack 3 `GCN_Layer` in order to construct a full Graph Neural Network. The GNN is followed by a `Linear` layer that will output the final classification between the 2 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                    in_features = 7,\n",
    "                    hidden_dim = 64,\n",
    "                    classes = 2,\n",
    "                    dropout = 0.5):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCN_Layer(in_features, hidden_dim)\n",
    "        self.conv2 = GCN_Layer(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCN_Layer(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x,A):\n",
    "        x = self.conv1(x, A)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, A)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, A)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # aggregate node embeddings\n",
    "        x = x.mean(dim=1)\n",
    "        # final classification layer\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-hospital",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Finally, we have our training loop. As you can see, there is nothing out of the ordinary here. Standard training code as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = 'cpu'\n",
    "\n",
    "print(f'Training on {device}')\n",
    "model = GNN(in_features = 7,\n",
    "                hidden_dim = 128,\n",
    "                classes = 2).to(device)\n",
    "\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader: \n",
    "        optimizer.zero_grad()  \n",
    "        X, A, labels = data\n",
    "        X, A, labels = X.to(device), A.to(device), labels.to(device)  \n",
    "        # Forward pass.\n",
    "        out = model(X, A)  \n",
    "        # Compute the graph classification loss.\n",
    "        loss = criterion(out, labels) \n",
    "        # Calculate gradients.\n",
    "        loss.backward()  \n",
    "        # Updates the models parameters\n",
    "        optimizer.step() \n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        X,A, labels = data\n",
    "        # Forward pass.\n",
    "        out = model(X, A)  \n",
    "        # Take the index of the class with the highest probability.\n",
    "        pred = out.argmax(dim=1) \n",
    "        # Compare with ground-truth labels.\n",
    "        correct += int((pred == labels).sum()) \n",
    "    return correct / len(loader.dataset)  \n",
    "\n",
    "best_val = -1\n",
    "for epoch in range(1, 241):\n",
    "    train(train_dataset)\n",
    "    train_acc = test(train_dataset)\n",
    "    val_acc = test(val_dataset)\n",
    "    if val_acc>best_val:\n",
    "        best_val = val_acc\n",
    "        epoch_best = epoch\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f} || Best Val Score: {best_val:.4f} (Epoch {epoch_best:03d}) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-prize",
   "metadata": {},
   "source": [
    "### GCN Layer solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. BUILD YOUR GCN LAYER\n",
    "class GCN_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple GCN layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        A: adjÎ±cency matrix\n",
    "        X: graph signal\n",
    "        \"\"\"\n",
    "        L = create_graph_lapl_norm(A)\n",
    "        x = self.linear(X)\n",
    "        return torch.bmm(L, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
