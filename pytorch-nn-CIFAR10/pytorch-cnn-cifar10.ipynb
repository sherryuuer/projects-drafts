{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "natural-springer",
   "metadata": {},
   "source": [
    "# Build and train a Convolutional Neural Network\n",
    "\n",
    "The below code should seem familiar to you as it's very similar with the one we used when training a feedforward network. The difference is that this time we have a CNN. \n",
    "\n",
    "Once again, we will import all the necessary libraries along with our pet images dataset. The images will be transformed to tensors and will be normalized\n",
    "\n",
    "The CNN structure will be:\n",
    "\n",
    "- A conv layer with 3 channels as input, 6 channels as output, and a 5x5 kernel\n",
    "- A 2x2 max-pooling layer\n",
    "- A conv layer with 6 channels as input, 16 channels as output, and a 5x5 kernel\n",
    "- A linear layer with 1655 nodes\n",
    "- A linear layer with 120 nodes\n",
    "- A linear layer with 84 nodes\n",
    "- A linear layer with 10 nodes\n",
    "- ReLUs a lot except the output layer\n",
    "\n",
    "Use Vanilla SGD once again with a learning rate of 0.001 and a momentum of 0.9, and the cross-entropy loss for our loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "#add CIFAR10 data in the environment\n",
    "sys.path.append(cwd + '/../cifar10')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import utils\n",
    "from torchvision import transforms\n",
    "#CIFAR10 is a custom Dataloader that loads a subset ofthe data from a local folder\n",
    "from Cifar10Dataloader import CIFAR10\n",
    "\n",
    "batch_size=4\n",
    "\n",
    "def show_image(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    #convert the images to tensor and normalized them\n",
    "    transform = transforms.Compose([\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    trainset = CIFAR10(root='../cifar10',  transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=False, num_workers=1)\n",
    "    return trainloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-softball",
   "metadata": {},
   "source": [
    "## Define a CNN\n",
    "\n",
    "Again the strcture:\n",
    "\n",
    "- A conv layer with 3 channels as input, 6 channels as output, and a 5x5 kernel\n",
    "- A 2x2 max-pooling layer\n",
    "- A conv layer with 6 channels as input, 16 channels as output, and a 5x5 kernel\n",
    "- A linear layer with 16x5x5 nodes\n",
    "- A linear layer with 120 nodes\n",
    "- A linear layer with 84 nodes\n",
    "- A linear layer with 10 nodes\n",
    "\n",
    "The trickiest part when building CNNs is to find the correct dimensions for each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. DEFINE THE CNN HERE\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ) # output size \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-mountain",
   "metadata": {},
   "source": [
    "## Train a CNN\n",
    "\n",
    "Build the training loop and execute a few epochs:\n",
    "\n",
    "- unwrap the inpu and labels\n",
    "- develop the forward and backward pass\n",
    "- print the loss every 2000 mini-batches (optionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. TRAIN THE MODEL HERE\n",
    "def train(model, training_data):\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "   \n",
    "    for epoch in range(1):  # loop over the dataset multiple times, here only give it 1\n",
    "\n",
    "        for i, data in enumerate(training_data, 0):\n",
    "            # get the inputs; cifar10 is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    dataiter = iter(load_data())\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # print images\n",
    "    show_image(utils.make_grid(images))\n",
    "    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "    \n",
    "    outputs = model(images)\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    training_data = load_data()\n",
    "\n",
    "    model = CNN()\n",
    "\n",
    "    train(model, training_data)\n",
    "    \n",
    "    evaluate(model)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-survival",
   "metadata": {},
   "source": [
    "## BTW: another solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. DEFINE THE CNN \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "#2. TRAIN THE MODEL \n",
    "def train(model, training_data):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "        for i, data in enumerate(training_data, 0):\n",
    "            # get the inputs; cifar10 is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
