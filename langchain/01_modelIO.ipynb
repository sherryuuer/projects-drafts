{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e5accc-adc3-472a-ac7a-dd7838efca34",
   "metadata": {},
   "source": [
    "### Set up env\n",
    "\n",
    "include the apikey and install parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba41ceb-2ca5-4321-bb70-1011e2f6dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d7eb62-9a5b-4514-ad78-7f3d64e7e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the api key\n",
    "# import openai\n",
    "import os\n",
    "# setup environ\n",
    "os.environ[\"openaikey\"] = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa101b31-ad84-4990-8532-ae38197e3b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mykey = os.getenv(\"openaikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fbbeb6-06b0-47ac-9352-be13b300658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test with openai api \n",
    "# openai.api_key = mykey\n",
    "# response = openai.Completion.create(\n",
    "#     model = \"model from open ai\",\n",
    "#     prompt = \"Some questions\",\n",
    "#     max_tokens = 300,\n",
    "# )\n",
    "# print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1535aa0-7f95-40c5-b094-dbbdab7c398c",
   "metadata": {},
   "source": [
    "### Try the langchain api with openai llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d4da5-3f15-4475-8118-ed668e5c06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use langchain to connect to openai\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(openai_api_key=mykey)\n",
    "print(llm(\"Here is a fun fact about nintendo: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b231f-0094-4cf2-9df0-d6f65d239e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you can pass the prompts as a list\n",
    "result = llm.generate([\n",
    "    \"question1: \",\n",
    "    \"question2: \"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ceaed5-3edb-462d-aae6-073591690ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.schema()\n",
    "# check the llm ouput from the json result\n",
    "result.llm_output\n",
    "# get the text part from the generations json result\n",
    "print(result.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21dd0e1-45a7-4886-ba35-8a4754715d5a",
   "metadata": {},
   "source": [
    "### Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9cc0c1-53ad-4237-aa3f-486618dc4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different way to do this from document.\n",
    "# !pip install langchain-openai\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# chat = ChatOpenAI(openai_api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c12a66-da9d-4149-b845-44ad10c27a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(openai_api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435487b-5f07-4631-9133-90df1c96a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "result = chat([\n",
    "    SystemMessage(content=\"You are a xxx person.\"),\n",
    "    HumanMessage(content=\"Tell me the fact about nintendo.\")\n",
    "])\n",
    "# result is a AIMessage object\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c2d60-7a84-4477-b3aa-ee59946ea68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate way\n",
    "result = chat.generate(\n",
    "    [SystemMessage(content=\"You are a xxx person.\"),\n",
    "     HumanMessage(content=\"Tell me the fact about nintendo.\")],\n",
    "    [SystemMessage(content=\"You are a xxx person.\"),\n",
    "     HumanMessage(content=\"Tell me the fact about capcon.\")],\n",
    ")\n",
    "# result is results list, get the first answer (a list)'s text\n",
    "result.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f43bb2-9c02-4217-9b55-d4326e8fd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat(\n",
    "    [SystemMessage(content=\"You are a xxx person.\"),\n",
    "     HumanMessage(content=\"Tell me the fact about capcon.\")],\n",
    "    temperature=2,\n",
    "    presence_penalty=2,\n",
    "    max_token=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5c99a-4de4-486e-a321-d62441a10b28",
   "metadata": {},
   "source": [
    "### Cache the answer! because it cost money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0941320-aedf-470a-901c-9ba03c69f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa3469-8f8d-44f0-bf1c-2096b5b7fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(\"Tell me a fact about capcon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf96367-9dda-4ad1-ab37-426df62186af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second time when run the same predict, it is very fast because it will check memory first\n",
    "llm.predict(\"Tell me a fact about capcon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d275e84-f435-421b-8636-1a9e43996df2",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "llm和chat/chat相对比较复杂分为三个对象，AI，human，system有各自的语言。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fbf2d-d4d3-4191-b587-6d967ca1cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "single_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me something about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e38aa-3060-4ed3-a163-9c74b38e23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(single_input_prompt.format(topic=\"the ocean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038b21d-5b49-42c1-8a59-cae709fb43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"company\"],\n",
    "    template=\"Tell me something about {topic} of {company}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8a403-7cdf-4cea-a08d-984796e78635",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(multi_input_prompt.format(topic=\"the ocean\", company=\"capcon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3d58d-f32a-4543-ba65-64d578c174d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat model\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d1e1c-6cfa-416f-aae7-f55f3c393b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the system message\n",
    "system_template=\"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "# define the human message\n",
    "human_template=\"{recipe_request}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# define the chat_prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a2400-dd7c-4edf-a762-6646f526a97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the prompt object attribute\n",
    "system_message_prompt.input_variables  # ['cooking_time', 'dietary_preference']\n",
    "human_message_prompt.input_variables  # ['recipe_request']\n",
    "chat_prompt.input_variables  # ['dietary_preference', 'cooking_time', 'recipe_request']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b2ccf-f776-46e5-af0a-d36428b5e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a chat completion from the formatted messages\n",
    "request = chat_prompt.format_prompt(\n",
    "    cooking_time=\"15 min\", \n",
    "    dietary_preference=\"Vegan\", \n",
    "    recipe_request=\"Quick Snack\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f02747-972f-43cc-9827-65fbee8ff3c0",
   "metadata": {},
   "source": [
    "Note the 3 from methods：\n",
    "\n",
    "- chat_prompt = ChatPromptTemplate.from_messages(list of message_prompts)\n",
    "- message_prompts = HumanMessagePromptTemplate.from_template & SystemMessagePromptTemplate.from_template\n",
    "- define templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c9c58-735c-4c15-a52c-c0c329520607",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat(request)  # result is a python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cae0b-8a26-4cf4-aee6-02817ea6bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)  # get the content attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2ba01-938c-4b05-a9f2-ec84f53c8072",
   "metadata": {},
   "source": [
    "### Build a simple func with model IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36ebce-7ba3-4536-8bf7-b118ddbc9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def travel_idea(interest, budget):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        interest: A str interest or hobby (e.g. fishing)\n",
    "        budget: A str budget (e.g. $10,000)\n",
    "    '''\n",
    "    # chat model\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    )\n",
    "\n",
    "    chat = ChatOpenAI(openai_api_key=mykey)\n",
    "    # define the system message\n",
    "    system_template = \"You are a travel agent that help people\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    # define the human message\n",
    "    human_template=\"Help me plan trips about {interest} on budget of {budget}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    # define the chat_prompt\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    # get a chat completion from the formatted messages\n",
    "    request = chat_prompt.format_prompt(\n",
    "        interest=interest, \n",
    "        budget=budget, \n",
    "    ).to_messages()\n",
    "    result = chat(request) \n",
    "    return result.content\n",
    "\n",
    "print(travel_idea('fishing','$10,000'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640e2c2-0878-4a7d-981d-429a5ddf91a3",
   "metadata": {},
   "source": [
    "### Few shot prompt template\n",
    "flow:\n",
    "1. system_message_prompt by `SystemMessagePromptTemplate.from_template`\n",
    "2. example_input by `HumanMessagePromptTemplate.from_template`\n",
    "3. example_output by `AIMessagePromptTemplate.from_template`\n",
    "4. human_mesage_prompt by `HumanMessagePromptTemplate.from_template`\n",
    "5. chat_prompt by `ChatPromptTemplate.from_messages` by compile 1,2,3,4\n",
    "6. get chat completion by `request = chat_prompt.format_prompt` and `to_messages()`\n",
    "7. get result by `chat(request)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9f13b-5d22-4f43-bbc2-1aaafcfd598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates complex legal terms into plain and understandable terms.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "legal_text = \"The provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, illegal, or unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall remain in full force and effect to the maximum extent permitted by law.\"\n",
    "example_input_one = HumanMessagePromptTemplate.from_template(legal_text)\n",
    "\n",
    "plain_text = \"The rules in this agreement can be separated. If a court decides that one rule or part of it is not valid, illegal, or cannot be enforced, the other rules will still apply and be enforced as much as they can under the law.\"\n",
    "example_output_one = AIMessagePromptTemplate.from_template(plain_text)\n",
    "\n",
    "human_template = \"{legal_text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, example_input_one, example_output_one, human_message_prompt]\n",
    ")\n",
    "\n",
    "some_example_text = \"The grantor, being the fee simple owner of the real property herein described, conveys and warrants to the grantee, his heirs and assigns, all of the grantor's right, title, and interest in and to the said property, subject to all existing encumbrances, liens, and easements, as recorded in the official records of the county, and any applicable covenants, conditions, and restrictions affecting the property, in consideration of the sum of [purchase price] paid by the grantee.\"\n",
    "request = chat_prompt.format_prompt(legal_text=some_example_text).to_messages()\n",
    "\n",
    "result = chat(request)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8eae3-a924-4d81-bd32-8b6c6c9fc5b1",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "- List parser by commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dcb83b-136e-4e71-8549-760e454d3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "api_key = open(\"C://Users//Marcial//Desktop//desktop_openai.txt\").read()\n",
    "model = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325669d-8f4e-4171-845b-8fc72906b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import parsers\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# see what is in the instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)  # Your response should be a list of comma separated values, eg: `foo, bar, baz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2ad61-4f76-4715-97c7-818170c885d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "reply = \"one, two, three\"\n",
    "output_parser.parse(\"one, two, three\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28cf04-9639-40f2-960d-81c5dd5f8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = '{request}\\n{format_instructions}'\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "request = chat_prompt.format_prompt(request=\"give me 5 characteristics of dogs\",\n",
    "                   format_instructions = output_parser.get_format_instructions()).to_messages()\n",
    "result = model(request)\n",
    "# Convert to desired output:\n",
    "output_parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45992a1-5d17-467e-bb37-344a360758fe",
   "metadata": {},
   "source": [
    "- Datatime parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e580b82-f82f-474f-b630-b04b1040e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "output_parser = DatetimeOutputParser()\n",
    "\n",
    "print(output_parser.get_format_instructions())\n",
    "# Write a datetime string that matches the following pattern: \"%Y-%m-%dT%H:%M:%S.%fZ\". Examples: 0976-07-27T05:21:47.523819Z, 1255-06-12T16:16:30.837257Z, 1427-12-23T03:26:22.664795Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121191b-259c-49af-a542-a414ba4be659",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{request}\\n{format_instructions}\"\n",
    "human_prompt=HumanMessagePromptTemplate.from_template(template_text)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "request = chat_prompt.format_prompt(request=\"What date was the 13th Amendment ratified in the US?\",\n",
    "                   format_instructions=output_parser.get_format_instructions()\n",
    "                   ).to_messages()\n",
    "result = model(request,\n",
    "              temperature=0)\n",
    "output_parser.parse(result.content)\n",
    "# this will get a error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc68e2-39f7-42b4-8a46-11811680fc52",
   "metadata": {},
   "source": [
    "- fix the error above by system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0186db-51e0-4372-97d9-a428352cc0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\"You always reply to questions only in datetime patterns.\")\n",
    "template_text = \"{request}\\n{format_instructions}\"\n",
    "human_prompt=HumanMessagePromptTemplate.from_template(template_text)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt,human_prompt])\n",
    "request = chat_prompt.format_prompt(request=\"What date was the 13th Amendment ratified in the US?\",\n",
    "                   format_instructions=output_parser.get_format_instructions()\n",
    "                   ).to_messages()\n",
    "result = model(request,temperature=0)\n",
    "output_parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8a451-7fd8-4113-b946-c4ab238da384",
   "metadata": {},
   "source": [
    "- Auto fix parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f477a63-a2c8-4b12-baa0-9e83d68ce5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "output_parser = DatetimeOutputParser()\n",
    "\n",
    "misformatted = result.content\n",
    "new_parser = OutputFixingParser.from_llm(parser=output_parser, llm=model) # llm = the model name\n",
    "new_parser.parse(misformatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b8aaa-b820-4d93-8451-5697908edb87",
   "metadata": {},
   "source": [
    "- Pydantic Json parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02cc02-d5aa-4f5f-b726-7ecb4ff78f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4a3a5-4d42-493a-92b6-e9c8ad0302ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "class Scientist(BaseModel):\n",
    "    \n",
    "    name: str = Field(description=\"Name of a Scientist\")\n",
    "    discoveries: list = Field(description=\"Python list of discoveries\")\n",
    "\n",
    "query = 'Name a famous scientist and a list of their discoveries' \n",
    "parser = PydanticOutputParser(pydantic_object=Scientist)\n",
    "print(parser.get_format_instructions())\n",
    "# The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "# As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
    "# the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "# Here is the output schema:\n",
    "# ```\n",
    "# {\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Name of a Scientist\", \"type\": \"string\"}, \"discoveries\": {\"title\": \"Discoveries\", \"description\": \"Python list of discoveries\", \"type\": \"array\", \"items\": {}}}, \"required\": [\"name\", \"discoveries\"]}\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0523b9-b9d3-4e23-b465-26e185fda96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=\"Tell me about a famous scientist\")\n",
    "\n",
    "output = model(_input.to_string())\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25dd3f-4daf-472f-9010-3715c1dcbe88",
   "metadata": {},
   "source": [
    "### Save and load prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c08ebc-4b8b-4d6d-994f-2bd44a342d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"Question: {question}\\n\\nAnswer: Let's think step by step.\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt.save(\"prompt.json\")\n",
    "\n",
    "# All prompts are loaded through the `load_prompt` function.\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loaded_prompt = load_prompt('prompt.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043482fc-dd08-4e71-a6e3-071fcd2e125b",
   "metadata": {},
   "source": [
    "### History Quiz\n",
    "\n",
    "Our main goal is to use LangChain and Python to create a very simple class with a few methods for:\n",
    "* Writing a historical question that has a date as the correct answer\n",
    "* Getting the correct answer from LLM\n",
    "* Getting a Human user's best guess at at correct answer\n",
    "* Checking/reporting the difference between the correct answer and the user answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f425c2b-272a-4a85-b1a1-f9b37765ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from datetime import datetime\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "f = open('C:\\\\Users\\\\Marcial\\\\Desktop\\\\desktop_openai.txt')\n",
    "api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ebdbe5-fd0a-4316-be61-efb4c99b9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryQuiz():\n",
    "\n",
    "    def create_history_question(self,topic):\n",
    "        '''\n",
    "        This method should output a historical question about the topic that has a date as the correct answer.\n",
    "        For example:\n",
    "        \n",
    "            \"On what date did World War 2 end?\"\n",
    "            \n",
    "        '''\n",
    "        # PART ONE: SYSTEM\n",
    "        system_template=\"You write single quiz questions about {topic}. You only return the quiz question.\"\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "        # PART TWO: HUMAN REQUEST\n",
    "        human_template=\"{question_request}\"\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "        # PART THREE: COMPILE TO CHAT\n",
    "        chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "        # PART FOUR: INSERT VARIABLES\n",
    "        request = chat_prompt.format_prompt(topic=topic,question_request=\"Give me a quiz question where the correct answer is a specific date.\").to_messages()\n",
    "        # PART FIVE: CHAT REQUEST\n",
    "        chat = ChatOpenAI(openai_api_key=api_key)\n",
    "        result = chat(request)\n",
    "        \n",
    "        # return a quiz （question which will be passed to other functions）\n",
    "        return result.content\n",
    "    \n",
    "    def get_AI_answer(self,question):\n",
    "        '''\n",
    "        This method should get the answer to the historical question from the method above.\n",
    "        Note: This answer must be in datetime format! Use DateTimeOutputParser to confirm!\n",
    "        \n",
    "        September 2, 1945 --> datetime.datetime(1945, 9, 2, 0, 0)\n",
    "        '''\n",
    "        # Datetime Parser\n",
    "        output_parser = DatetimeOutputParser()\n",
    "        \n",
    "        # SYSTEM Template\n",
    "        system_template = \"You answer quiz questions with just a date.\"\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "        \n",
    "        \n",
    "        # HUMAN Template\n",
    "        human_template = \"\"\"Answer the user's question:\n",
    "        \n",
    "        {question}\n",
    "        \n",
    "        {format_instructions}\"\"\"\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "        \n",
    "        # Compile ChatTemplate\n",
    "        chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])\n",
    "        \n",
    "        # Insert question and format instructions\n",
    "        \n",
    "        request = chat_prompt.format_prompt(question=question,\n",
    "                                            format_instructions=output_parser.get_format_instructions()).to_messages()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Chat Bot\n",
    "        chat = ChatOpenAI(openai_api_key=api_key)\n",
    "        result = chat(request)\n",
    "        # Format Request to datetime\n",
    "        correct_datetime = output_parser.parse(result.content)\n",
    "        return correct_datetime\n",
    "    \n",
    "    def get_user_answer(self,question):\n",
    "        '''\n",
    "        This method should grab a user answer and convert it to datetime. It should collect a Year, Month, and Day.\n",
    "        You can just use input() for this.\n",
    "        '''\n",
    "        print(question)\n",
    "        \n",
    "\n",
    "        # Get the year, month, and day from the user\n",
    "        year = int(input(\"Enter the year: \"))\n",
    "        month = int(input(\"Enter the month (1-12): \"))\n",
    "        day = int(input(\"Enter the day (1-31): \"))\n",
    "\n",
    "        # Create a datetime object\n",
    "        user_datetime = datetime(year, month, day)\n",
    "\n",
    "        \n",
    "        return user_datetime\n",
    "        \n",
    "        \n",
    "    def check_user_answer(self,user_answer,ai_answer):\n",
    "        '''\n",
    "        Should check the user answer against the AI answer and return the difference between them\n",
    "        '''\n",
    "        \n",
    "        # Calculate the difference between the dates\n",
    "        difference = user_answer - ai_answer\n",
    "\n",
    "        # Format the difference into a string\n",
    "        formatted_difference = str(difference)\n",
    "\n",
    "        # Return the string reporting the difference\n",
    "        print(\"The difference between the dates is:\", formatted_difference)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49fbd9-3fc6-45b2-bf73-a5f645160b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_bot = HistoryQuiz()\n",
    "question = quiz_bot.create_history_question(topic='World War 2')\n",
    "ai_answer = quiz_bot.get_AI_answer(question)\n",
    "user_answer = quiz_bot.get_user_answer(question)\n",
    "quiz_bot.check_user_answer(user_answer,ai_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
