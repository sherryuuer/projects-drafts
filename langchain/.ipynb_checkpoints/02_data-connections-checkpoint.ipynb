{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1f1c6d-8d58-4b14-b875-ba47df891549",
   "metadata": {},
   "source": [
    "### Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249dfd98-f2f5-4b75-a161-49b32ed36acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv\n",
    "from langchain.document_loaders import CSVLoader\n",
    "loader = CSVLoader('some_data/penguins.csv')\n",
    "data = loader.load()\n",
    "print(data)  # a list\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1f973-ab35-4224-baba-a608edd3a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html\n",
    "!pip install beautifulsoup4\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "loader = BSHTMLLoader('some_data/some_website.html')\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fa990-7cbd-472a-beba-9ce38f8644e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf\n",
    "!pip install pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('some_data/SomeReport.pdf')\n",
    "pages = loader.load_and_split()\n",
    "type(pages)  # list\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d319e-2030-463f-bee6-ffb6c64c84a2",
   "metadata": {},
   "source": [
    "### Integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8d411-1b1a-4f72-87bc-aa9e0cdef00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader  # Hacker News\n",
    "loader = HNLoader(link_of_hacker_news)\n",
    "data = loader.load()\n",
    "print(data[0].page_content, data[0].metadata)\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "f = open('key.txt')\n",
    "api_key = f.read()\n",
    "model = ChatOpenAI(openai_api_key=api_key)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Please give me a single sentence summary of the following:\\n{document}')\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "result = model(chat_prompt.format_prompt(document=data[0].page_content).to_messages())\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65437986-3fcf-4b78-acfd-d01ce06fad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: user wiki page to anwser\n",
    "# !pip install wikipedia\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "\n",
    "def answer_question_about(person_name,question):\n",
    "    # Get Wikipedia Article\n",
    "    docs = WikipediaLoader(query=person_name, load_max_docs=1)\n",
    "    context_text = docs.load()[0].page_content\n",
    "    \n",
    "    # Connect to OpenAI Model\n",
    "    f = open('keyfile.txt')\n",
    "    api_key = f.read()\n",
    "    model = ChatOpenAI(openai_api_key=api_key)\n",
    "    \n",
    "    # Ask Model Question\n",
    "    human_prompt = HumanMessagePromptTemplate.from_template('Answer this question\\n{question}, here is some extra context:\\n{document}')\n",
    "    \n",
    "    # Assemble chat prompt\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "    \n",
    "    #result\n",
    "    result = model(chat_prompt.format_prompt(question=question, document=data[0].page_content).to_messages())\n",
    "    \n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d2dca-02ac-4f5e-8bd1-6e960ad7745f",
   "metadata": {},
   "source": [
    "### Document Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3366098-ed31-4970-b79e-63d31f17cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('some_data/FDR_State_of_Union_1944.txt') as file:\n",
    "    speech_text = file.read()\n",
    "\n",
    "# split by character\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=1000) #1000 is default value\n",
    "texts = text_splitter.create_documents([speech_text])\n",
    "print(type(texts)) # list\n",
    "print('\\n')\n",
    "print(texts[0].page_content)\n",
    "type(texts[0]) # langchain.schema.document.Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147668d1-57a3-46fc-a306-e88ff4cbd18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by token\n",
    "# !pip install tiktoken\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500) #now chunk size is a hard length based on tokens\n",
    "texts = text_splitter.split_text(speech_text) # texts is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7de049-7e89-40bb-92a5-6f81d547f8a1",
   "metadata": {},
   "source": [
    "### Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab8fa2-a95e-49f3-8a24-515c2584ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "f = open('key.txt')\n",
    "os.environ['OPENAI_API_KEY'] = f.read()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text = \"Some normal text to send to OpenAI to be embedded into a N dimensional vector\"\n",
    "embedded_text = embeddings.embed_query(text)\n",
    "type(embedded_text)  # list \n",
    "\n",
    "# Embeded document\n",
    "from langchain.document_loaders import CSVLoader\n",
    "loader = CSVLoader('some_data/penguins.csv')\n",
    "data = loader.load()\n",
    "type(data)  # list\n",
    "type(data[0])  # langchain.schema.document.Document\n",
    "embedded_docs = embeddings.embed_documents([text.page_content for text in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e90c7-8e63-4dae-b33d-e103a6941c2a",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb2df0-c458-44b0-91aa-5e1a1dbe00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE ARE THE VERSION NUMBERS THAT WORKED FOR ME:\n",
    "# CAREFUL WITH PYDANTIC, DO IT LAST SINCE CHROMA AND LANGCHAIN AUTO INSTALL IT AS A DEPENDENCY\n",
    "# Use this to install specific versions numbers:\n",
    "# !pip install package_name==0.3.26\n",
    "import chromadb\n",
    "print(chromadb.__version__)\n",
    "import langchain\n",
    "print(langchain.__version__)\n",
    "import pydantic\n",
    "print(pydantic.__version__)\n",
    "\n",
    "import chromadb\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"some_data/FDR_State_of_Union_1944.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "import os\n",
    "f = open('key.txt')\n",
    "os.environ['OPENAI_API_KEY'] = f.read()\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function, persist_directory='./speech_embedding_db')\n",
    "\n",
    "# Helpful to force a save\n",
    "db.persist()\n",
    "\n",
    "# Load Embeddings from db\n",
    "db_connection = Chroma(persist_directory='./speech_embedding_db/', embedding_function=embedding_function)\n",
    "\n",
    "# Get a similarity search\n",
    "new_doc = \"What did FDR say about the cost of food law?\"\n",
    "docs = db_connection.similarity_search(new_doc)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# Add document to db\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"some_data/Lincoln_State_of_Union_1862.txt\")\n",
    "documents = loader.load()\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function,persist_directory='./speech_embedding_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3011282-98ec-4177-9d3a-b29489ece939",
   "metadata": {},
   "source": [
    "### Vector Store Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e0ede6-a138-4391-8c58-c9a8ca1a16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_connection.as_retriever()\n",
    "search_kwargs = {\"score_threshold\":0.8, \"k\":4}\n",
    "docs = retriever.get_relevant_documents(\"President\",\n",
    "                                       search_kwargs=search_kwargs)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df286c24-ebac-411d-be68-7ac890b87e89",
   "metadata": {},
   "source": [
    "### Use Chat Model to Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aba59e-d8c4-420f-bd8e-8696c6d6375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "question = \"When was this declassified?\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db.as_retriever(), llm=llm)\n",
    "\n",
    "# Set logging for the queries\n",
    "# import logging\n",
    "# logging.basicConfig()\n",
    "# logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "print(unique_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91945695-6c50-4719-8da3-095dd4bedbff",
   "metadata": {},
   "source": [
    "### Context Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badf059-71fb-4c2f-8499-cdb1527642d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=db_connection.as_retriever())\n",
    "docs = db_connection.similarity_search('When was this declassified?')\n",
    "print(docs[0])\n",
    "print(compressed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52ccb87-dd64-46de-b520-2fe57ee1df2e",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc462a-f662-4eb6-bcf9-e1893881e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a Legal Research Assistant Bot about the US Constitution\n",
    "\n",
    "# Takes in a question about the US Constitution and returns the most relevant\n",
    "# part of the constitution. Notice it may not directly answer the actual question!\n",
    "\n",
    "# PART ONE:\n",
    "# LOAD \"some_data/US_Constitution in a Document object\n",
    "loader = TextLoader(\"some_data/US_Constitution.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# PART TWO\n",
    "# Split the document into chunks (you choose how and what size)\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# PART THREE\n",
    "# EMBED THE Documents (now in chunks) to a persisted ChromaDB\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(docs, embedding_function, persist_directory='./US_Constitution')\n",
    "db.persist()\n",
    "\n",
    "def us_constitution_helper(question):\n",
    "    # PART FOUR\n",
    "    # Use ChatOpenAI and ContextualCompressionRetriever to return the most\n",
    "    # relevant part of the documents.\n",
    "\n",
    "    # results = db.similarity_search(\"What is the 13th Amendment?\")\n",
    "    # print(results[0].page_content) # NEED TO COMPRESS THESE RESULTS!\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, \n",
    "                                                           base_retriever=db.as_retriever())\n",
    "\n",
    "    compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "    return compressed_docs[0].page_content\n",
    "\n",
    "print(us_constitution_helper(\"What is the 13th Amendment?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
