{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3585743-5b5b-489e-baf7-e4d0d7946674",
   "metadata": {},
   "source": [
    "### Single LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6643f9-b842-4085-b60e-df770e23fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "f = open('key.txt')\n",
    "os.environ['OPENAI_API_KEY'] = f.read()\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe2c4d-a83f-45a1-a254-69d296ebcb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        \"Make up a funny company name for a company that produces {product}\"\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "\n",
    "print(chain.run(product=\"Computers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632cbe7d-0c35-4924-bc37-cc3591603121",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f17e5-b5be-4cd3-a130-889c20ef7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = \"Give me a simple bullet point outline for a blog post on {topic}\"\n",
    "first_prompt = ChatPromptTemplate.from_template(template)\n",
    "chain_one = LLMChain(llm=llm,prompt=first_prompt)\n",
    "\n",
    "template = \"Write a blog post using this outline: {outline}\"\n",
    "second_prompt = ChatPromptTemplate.from_template(template)\n",
    "chain_two = LLMChain(llm=llm,prompt=second_prompt)\n",
    "\n",
    "full_chain = SimpleSequentialChain(chains=[chain_one,chain_two],\n",
    "                                  verbose=True)\n",
    "result = full_chain.run(\"Data Science\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6c1b9-8e5c-4ab8-b819-26a8ef86e191",
   "metadata": {},
   "source": [
    "### Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b8021-6f98-4668-9d27-6e9a718aa700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template1 = \"Give a summary of this employee's performance review:\\n{review}\"\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "chain_1 = LLMChain(llm=llm,\n",
    "                     prompt=prompt1,\n",
    "                     output_key=\"review_summary\")\n",
    "\n",
    "template2 = \"Identify key employee weaknesses in this review summary:\\n{review_summary}\"\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "chain_2 = LLMChain(llm=llm,\n",
    "                     prompt=prompt2,\n",
    "                     output_key=\"weaknesses\")\n",
    "\n",
    "template3 = \"Create a personalized plan to help address and fix these weaknesses:\\n{weaknesses}\"\n",
    "prompt3 = ChatPromptTemplate.from_template(template3)\n",
    "chain_3 = LLMChain(llm=llm,\n",
    "                     prompt=prompt3,\n",
    "                     output_key=\"final_plan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2b349-3380-4d84-8714-21d1792e5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = SequentialChain(chains=[chain_1,chain_2,chain_3],\n",
    "                            input_variables=['review'],\n",
    "                            output_variables=['review_summary','weaknesses','final_plan'],\n",
    "                            verbose=True)\n",
    "\n",
    "results = seq_chain(employee_review)\n",
    "print(results['final_plan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108d452-ef4d-4f29-be64-2b33ccae0caf",
   "metadata": {},
   "source": [
    "### LLMRouterChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9831e-fd62-4974-b274-8744b02f36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route Templates\n",
    "beginner_template = '''You are a physics teacher who is really\n",
    "focused on beginners and explaining complex topics in simple to understand terms. \n",
    "You assume no prior knowledge. Here is the question\\n{input}'''\n",
    "expert_template = '''You are a world expert physics professor who explains physics topics\n",
    "to advanced audience members. You can assume anyone you answer has a \n",
    "PhD level understanding of Physics. Here is the question\\n{input}'''\n",
    "\n",
    "# Route Prompts\n",
    "prompt_infos = [\n",
    "    {'name':'advanced physics','description': 'Answers advanced physics questions',\n",
    "     'prompt_template':expert_template},\n",
    "    {'name':'beginner physics','description': 'Answers basic beginner physics questions',\n",
    "     'prompt_template':beginner_template},  \n",
    "]\n",
    "\n",
    "# ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm,prompt=default_prompt)\n",
    "\n",
    "# Routing Destinations\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "# Router Prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8777c1-9acc-4139-8e30-6bc3729157f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing Chain CallÂ¶\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True)\n",
    "# another version from document: https://www.langchain.asia/modules/chains/examples/multi_prompt_router\n",
    "# chain = MultiPromptChain.from_prompts(OpenAI(), prompt_infos, verbose=True)\n",
    "chain.run(\"How do magnets work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600433fd-39bf-4e81-b500-337aa0bba518",
   "metadata": {},
   "source": [
    "### TransformChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251aa728-b12b-48e3-99b9-c3ed52944b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "yelp_review = open('yelp_review.txt').read()\n",
    "\n",
    "# custom function\n",
    "def transformer_fun(inputs: dict) -> dict:\n",
    "    '''\n",
    "    Notice how this always takes an inputs dictionary.\n",
    "    Also outputs a dictionary. You can call the output and input keys whatever you want, \n",
    "    just make sure to reference it correct in the chain call.\n",
    "    '''\n",
    "    # GRAB INCOMING CHAIN TEXT\n",
    "    text = inputs['text']\n",
    "    only_review_text = text.split('REVIEW:')[-1]\n",
    "    lower_case_text = only_review_text.lower()\n",
    "    return {'output':lower_case_text}\n",
    "\n",
    "transform_chain = TransformChain(input_variables=['text'],\n",
    "                                 output_variables=['output'],\n",
    "                                 transform=transformer_fun)\n",
    "template = \"Create a one sentence summary of this review:\\n{review_text}\"\n",
    "summary_chain = LLMChain(llm=llm,\n",
    "                         prompt=prompt,\n",
    "                         output_key=\"review_summary\")\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain,summary_chain],\n",
    "                                        verbose=True)\n",
    "result = sequential_chain(yelp_review)\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55127b5f-e367-4fb6-bda4-4b8548202757",
   "metadata": {},
   "source": [
    "### Using OpenAI Functions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50391682-74fc-4b43-8428-f17578e5260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Dog(BaseModel):\n",
    "    \"\"\"Identifying information about a dog.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The dog's name\")\n",
    "    color: str = Field(..., description=\"The dog's color\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a world class algorithm for extracting information in structured formats.\"),\n",
    "        (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "        (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "    ]\n",
    ")\n",
    "chain = create_structured_output_runnable(Dog, llm, prompt)\n",
    "chain.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "# -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05afc4e-a1fa-4566-b12b-8c7d020f9e7b",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776bca1-eff1-439e-84de-60f8327d83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language --> Translate to English --> Summary the email\n",
    "spanish_email = open('spanish_customer_email.txt').read()\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain,SequentialChain\n",
    "\n",
    "def translate_and_summarize(email):\n",
    "    \"\"\"\n",
    "    Translates an email written in a detected language to English and generates a summary.\n",
    "\n",
    "    Args:\n",
    "        email (str): The email to be processed and translated.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - 'language': The language the email was written in.\n",
    "            - 'translated_email': The translated version of the email in English.\n",
    "            - 'summary': A short summary of the translated email.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs during the LLM chain execution.\n",
    "\n",
    "    Example:\n",
    "        email = \"Hola, Â¿cÃ³mo estÃ¡s? Espero que todo vaya bien.\"\n",
    "        result = translate_and_summarize(email)\n",
    "        print(result)\n",
    "        # Output:\n",
    "        # {\n",
    "        #     'language': 'Spanish',\n",
    "        #     'translated_email': 'Hello, how are you? I hope everything is going well.',\n",
    "        #     'summary': 'A friendly greeting and a wish for well-being.'\n",
    "        # }\n",
    "    \"\"\"\n",
    "    # Create Model\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    # CREATE A CHAIN THAT DOES THE FOLLOWING:\n",
    "    \n",
    "    # Detect Language\n",
    "    template1 = \"Return the language this email is written in:\\n{email}.\\nONLY return the language it was written in.\"\n",
    "    prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "    chain_1 = LLMChain(llm=llm,\n",
    "                     prompt=prompt1,\n",
    "                     output_key=\"language\")\n",
    "    \n",
    "    # Translate from detected language to English\n",
    "    template2 = \"Translate this email from {language} to English. Here is the email:\\n\"+email\n",
    "    prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "    chain_2 = LLMChain(llm=llm,\n",
    "                     prompt=prompt2,\n",
    "                     output_key=\"translated_email\")\n",
    "    \n",
    "    # Return English Summary AND the Translated Email\n",
    "    template3 = \"Create a short summary of this email:\\n{translated_email}\"\n",
    "    prompt3 = ChatPromptTemplate.from_template(template3)\n",
    "    chain_3 = LLMChain(llm=llm,\n",
    "                     prompt=prompt3,\n",
    "                     output_key=\"summary\")\n",
    "    \n",
    "    seq_chain = SequentialChain(chains=[chain_1,chain_2,chain_3],\n",
    "                            input_variables=['email'],\n",
    "                            output_variables=['language','translated_email','summary'],\n",
    "                            verbose=True)\n",
    "    return seq_chain(email)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
